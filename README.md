# Road to Interpretability

Welcome to **Road to Interpretability**, a project exploring why researchers aim to achieve mechanistic interpretability and a deep dive into **Anthropicâ€™s paper on mapping the mind of an LLM**. This project was created for my **Deep Learning class**, where I analyzed interpretability research and explained its significance in understanding large language models.

## ðŸ“– About This Project

In this project, I discuss:
- The motivation behind **mechanistic interpretability**â€”why researchers want to break down neural networks into understandable components.
- **Anthropicâ€™s work on interpreting LLMs**, focusing on their approach to mapping internal activations to human-interpretable concepts.
  
The project is hosted on **GitHub Pages**, and you can view it here:  
ðŸ”— [Road to Interpretability](https://natalieho1.github.io/RoadToInterpretability/)

## ðŸ“‚ Repository Structure

- **`index.html`** â€“ The main landing page for the project.
- **`README.md`** â€“ This file!

## ðŸš€ Getting Started

To run this project locally:
1. Clone the repository:
   ```bash
   git clone https://github.com/natalieho1/RoadToInterpretability.git

2. Open index.html in your browser 
